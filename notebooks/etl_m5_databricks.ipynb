{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c351f727-db8e-4530-b1d8-c82a4e811a07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f879ccb-0cff-420b-ae24-56c3b24c8f51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account = \"stgm5forecastdev\"\n",
    "\n",
    "account_key = dbutils.secrets.get(scope=\"m5-scope\", key=\"adls-key\")\n",
    "\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account}.dfs.core.windows.net\",\n",
    "    account_key\n",
    ")\n",
    "\n",
    "\n",
    "raw_path = f\"abfss://raw@{storage_account}.dfs.core.windows.net\"\n",
    "curated_path = f\"abfss://curated@{storage_account}.dfs.core.windows.net\"\n",
    "\n",
    "display(dbutils.fs.ls(raw_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc1c79bd-d1e3-44c1-b1c5-2393a68ca9a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#ETL steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c756029-bf57-44b3-9be5-dd5fe8d25878",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Smoke test ETL step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c9c95fd-5e1d-4f97-93e8-0799870a0c26",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"#row_number#\":52},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764117028243}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "file_path = f\"{raw_path}/sales_train_evaluation.csv\"\n",
    "\n",
    "df = (spark.read\n",
    "      .option(\"header\", \"true\")\n",
    "      .csv(file_path))\n",
    "\n",
    "# Keep id columns + first 7 days as integers for a smoke test\n",
    "id_cols = [\"id\", \"item_id\", \"dept_id\", \"store_id\", \"cat_id\", \"state_id\"]\n",
    "value_cols = [c for c in df.columns if c.startswith(\"d_\")][:7]\n",
    "\n",
    "df_small = df.select(\n",
    "    *id_cols,\n",
    "    *[col(c).cast(\"int\").alias(c) for c in value_cols]\n",
    ")\n",
    "\n",
    "output_path = f\"{curated_path}/sales_small/\"\n",
    "\n",
    "(df_small\n",
    " .write\n",
    " .mode(\"overwrite\")\n",
    " .parquet(output_path))\n",
    "\n",
    "display(dbutils.fs.ls(output_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21b85642-1dde-404d-8e50-8f0b7d100f8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Creating a long-format table for CA1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47951632-267f-489d-bda8-872e8e2d30e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, col\n",
    "\n",
    "storage_account = \"stgm5forecastdev\"\n",
    "raw_path = f\"abfss://raw@{storage_account}.dfs.core.windows.net\"\n",
    "curated_path = f\"abfss://curated@{storage_account}.dfs.core.windows.net\"\n",
    "\n",
    "# 1) Read full sales_train_evaluation\n",
    "df = (spark.read\n",
    "      .option(\"header\", \"true\")\n",
    "      .csv(f\"{raw_path}/sales_train_evaluation.csv\"))\n",
    "\n",
    "# 2) Filter to a single store to keep it light, e.g. CA_1\n",
    "df_store = df.filter(col(\"store_id\") == \"CA_1\")\n",
    "\n",
    "# 3) Unpivot d_1...d_1913 into (d, sales)\n",
    "value_cols = [c for c in df_store.columns if c.startswith(\"d_\")]\n",
    "\n",
    "exprs = \", \".join([f\"'{c}', {c}\" for c in value_cols])\n",
    "\n",
    "df_long = (df_store.select(\"id\", \"item_id\", \"dept_id\", \"store_id\", \"cat_id\", \"state_id\", *value_cols)\n",
    "           .selectExpr(\n",
    "               \"id\", \"item_id\", \"dept_id\", \"store_id\", \"cat_id\", \"state_id\",\n",
    "               f\"stack({len(value_cols)}, {exprs}) as (d, sales)\"\n",
    "           ))\n",
    "\n",
    "# 4) Cast sales to int; map d_# to a dummy date index (we'll map to real dates later if needed)\n",
    "df_long = df_long.withColumn(\"sales\", col(\"sales\").cast(\"int\"))\n",
    "\n",
    "# 5) Write to curated as Parquet\n",
    "output_path = f\"{curated_path}/m5_daily_ca1/\"\n",
    "(df_long\n",
    " .write\n",
    " .mode(\"overwrite\")\n",
    " .partitionBy(\"store_id\", \"item_id\")\n",
    " .parquet(output_path))\n",
    "\n",
    "display(dbutils.fs.ls(output_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c4a9478-b70c-4922-ac1a-b0c5c93dac77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Sanity-checking the long-table format for CA1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75cc0448-8a46-465f-9c44-9e38f46dcfb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "storage_account = \"stgm5forecastdev\"\n",
    "curated_path = f\"abfss://curated@{storage_account}.dfs.core.windows.net\"\n",
    "\n",
    "daily_path = f\"{curated_path}/m5_daily_ca1/\"\n",
    "\n",
    "df_daily = spark.read.parquet(daily_path)\n",
    "\n",
    "df_daily.printSchema()\n",
    "df_daily.show(5)\n",
    "df_daily.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dc53d3c-597e-4734-961a-63beb7add7ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_daily = df_daily.withColumn(\n",
    "    \"day_num\",\n",
    "    regexp_replace(\"d\", \"d_\", \"\").cast(\"int\")\n",
    ")\n",
    "\n",
    "df_daily.select(\"id\", \"store_id\", \"item_id\", \"d\", \"day_num\", \"sales\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "374bb7a8-8348-4d10-a13d-c2918b7043dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "item_example = df_daily.select(\"item_id\").first()[\"item_id\"]\n",
    "print(\"Using item:\", item_example)\n",
    "\n",
    "df_item = (df_daily\n",
    "           .filter(col(\"item_id\") == item_example)\n",
    "           .orderBy(\"day_num\"))\n",
    "\n",
    "pdf = df_item.select(\"day_num\", \"sales\").toPandas()\n",
    "pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9477e8ec-037e-4d38-a585-8dc11d8cdf02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    return 100 / len(y_true) * np.sum(\n",
    "        2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-8)\n",
    "    )\n",
    "\n",
    "pdf = pdf.dropna().sort_values(\"day_num\")\n",
    "\n",
    "horizon = 28  # last 28 days as validation\n",
    "train = pdf.iloc[:-horizon]\n",
    "val = pdf.iloc[-horizon:]\n",
    "\n",
    "X_train = train[[\"day_num\"]]\n",
    "y_train = train[\"sales\"]\n",
    "X_val = val[[\"day_num\"]]\n",
    "y_val = val[\"sales\"]\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"mae\",\n",
    "    \"verbosity\": -1,\n",
    "}\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    num_boost_round=200,\n",
    "    valid_sets=[val_data],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=20, verbose=False)],\n",
    ")\n",
    "\n",
    "y_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "print(\"SMAPE:\", smape(y_val, y_pred))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "etl_m5_databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
