{
    "cells": [
        {
            "cell_type": "code",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "c351f727-db8e-4530-b1d8-c82a4e811a07",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "source": [
                "%pip install lightgbm"
            ],
            "execution_count": 0,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "0f879ccb-0cff-420b-ae24-56c3b24c8f51",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "source": [
                "# Configuration - parameterized for reuse\n",
                "storage_account = \"stgm5forecastdev\"\n",
                "\n",
                "account_key = dbutils.secrets.get(scope=\"m5-scope\", key=\"adls-key\")\n",
                "\n",
                "spark.conf.set(\n",
                "    f\"fs.azure.account.key.{storage_account}.dfs.core.windows.net\",\n",
                "    account_key\n",
                ")\n",
                "\n",
                "raw_path = f\"abfss://raw@{storage_account}.dfs.core.windows.net\"\n",
                "curated_path = f\"abfss://curated@{storage_account}.dfs.core.windows.net\"\n",
                "\n",
                "display(dbutils.fs.ls(raw_path))\n"
            ],
            "execution_count": 0,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Add src to path for imports (Databricks Repos)\n",
                "import sys\n",
                "import os\n",
                "\n",
                "# Get the repo root (parent of notebooks/)\n",
                "repo_root = os.path.dirname(os.path.abspath(\".\"))\n",
                "if repo_root not in sys.path:\n",
                "    sys.path.insert(0, repo_root)\n",
                "\n",
                "# Import ETL functions from src\n",
                "from src.etl import transform_sales_to_long, write_parquet"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "7c756029-bf57-44b3-9be5-dd5fe8d25878",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "source": [
                "## Smoke Test ETL Step"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "9c9c95fd-5e1d-4f97-93e8-0799870a0c26",
                    "showTitle": false,
                    "tableResultSettingsMap": {
                        "0": {
                            "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"#row_number#\":52},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764117028243}",
                            "filterBlob": null,
                            "queryPlanFiltersBlob": null,
                            "tableResultIndex": 0
                        }
                    },
                    "title": ""
                }
            },
            "source": [
                "from pyspark.sql.functions import col\n",
                "\n",
                "file_path = f\"{raw_path}/sales_train_evaluation.csv\"\n",
                "\n",
                "df = (spark.read\n",
                "      .option(\"header\", \"true\")\n",
                "      .csv(file_path))\n",
                "\n",
                "# Keep id columns + first 7 days as integers for a smoke test\n",
                "id_cols = [\"id\", \"item_id\", \"dept_id\", \"store_id\", \"cat_id\", \"state_id\"]\n",
                "value_cols = [c for c in df.columns if c.startswith(\"d_\")][:7]\n",
                "\n",
                "df_small = df.select(\n",
                "    *id_cols,\n",
                "    *[col(c).cast(\"int\").alias(c) for c in value_cols]\n",
                ")\n",
                "\n",
                "output_path = f\"{curated_path}/sales_small/\"\n",
                "\n",
                "(df_small\n",
                " .write\n",
                " .mode(\"overwrite\")\n",
                " .parquet(output_path))\n",
                "\n",
                "display(dbutils.fs.ls(output_path))\n"
            ],
            "execution_count": 0,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "21b85642-1dde-404d-8e50-8f0b7d100f8a",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "source": [
                "## Creating a long-format table for CA1  "
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "47951632-267f-489d-bda8-872e8e2d30e7",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "source": [
                "# Main ETL: Transform sales data to long format using src/etl module\n",
                "# This replaces inline transformation logic with reusable functions\n",
                "\n",
                "STORE_ID = \"CA_1\"\n",
                "\n",
                "# Transform: read raw -> filter store -> wide to long -> add day_num -> join calendar\n",
                "df_long = transform_sales_to_long(\n",
                "    spark=spark,\n",
                "    raw_path=raw_path,\n",
                "    store_id=STORE_ID,\n",
                "    add_day_num=True,\n",
                "    calendar_path=raw_path,\n",
                ")\n",
                "\n",
                "# Write to curated zone\n",
                "output_path = f\"{curated_path}/m5_daily_{STORE_ID.lower()}/\"\n",
                "write_parquet(\n",
                "    df=df_long,\n",
                "    output_path=output_path,\n",
                "    partition_by=[\"store_id\", \"item_id\"],\n",
                "    mode=\"overwrite\"\n",
                ")\n",
                "\n",
                "display(dbutils.fs.ls(output_path))\n"
            ],
            "execution_count": 0,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "5c4a9478-b70c-4922-ac1a-b0c5c93dac77",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "source": [
                "## Sanity-checking the long-table format for CA1"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "75cc0448-8a46-465f-9c44-9e38f46dcfb9",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "source": [
                "# Read back curated data for sanity check\n",
                "daily_path = f\"{curated_path}/m5_daily_{STORE_ID.lower()}/\"\n",
                "df_daily = spark.read.parquet(daily_path)\n",
                "\n",
                "df_daily.printSchema()\n",
                "df_daily.show(5)\n",
                "print(f\"Total rows: {df_daily.count():,}\")\n"
            ],
            "execution_count": 0,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "4dc53d3c-597e-4734-961a-63beb7add7ed",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "source": [
                "# day_num is now included from ETL - just verify\n",
                "df_daily.select(\"id\", \"store_id\", \"item_id\", \"d\", \"day_num\", \"sales\").show(5)\n"
            ],
            "execution_count": 0,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "374bb7a8-8348-4d10-a13d-c2918b7043dc",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "source": [
                "from pyspark.sql.functions import col\n",
                "\n",
                "item_example = df_daily.select(\"item_id\").first()[\"item_id\"]\n",
                "print(\"Using item:\", item_example)\n",
                "\n",
                "df_item = (df_daily\n",
                "           .filter(col(\"item_id\") == item_example)\n",
                "           .orderBy(\"day_num\"))\n",
                "\n",
                "pdf = df_item.select(\"day_num\", \"sales\").toPandas()\n",
                "pdf.head()"
            ],
            "execution_count": 0,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "9477e8ec-037e-4d38-a585-8dc11d8cdf02",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "source": [
                "import lightgbm as lgb\n",
                "from src.model import smape  # Use SMAPE from src module\n",
                "\n",
                "pdf = pdf.dropna().sort_values(\"day_num\")\n",
                "\n",
                "horizon = 28  # last 28 days as validation\n",
                "train = pdf.iloc[:-horizon]\n",
                "val = pdf.iloc[-horizon:]\n",
                "\n",
                "X_train = train[[\"day_num\"]]\n",
                "y_train = train[\"sales\"]\n",
                "X_val = val[[\"day_num\"]]\n",
                "y_val = val[\"sales\"]\n",
                "\n",
                "train_data = lgb.Dataset(X_train, label=y_train)\n",
                "val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
                "\n",
                "params = {\n",
                "    \"objective\": \"regression\",\n",
                "    \"metric\": \"mae\",\n",
                "    \"verbosity\": -1,\n",
                "}\n",
                "\n",
                "model = lgb.train(\n",
                "    params,\n",
                "    train_data,\n",
                "    num_boost_round=200,\n",
                "    valid_sets=[val_data],\n",
                "    callbacks=[lgb.early_stopping(stopping_rounds=20, verbose=False)],\n",
                ")\n",
                "\n",
                "y_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
                "print(\"SMAPE:\", smape(y_val, y_pred))\n",
                "\n"
            ],
            "execution_count": 0,
            "outputs": []
        }
    ],
    "metadata": {
        "application/vnd.databricks.v1+notebook": {
            "computePreferences": null,
            "dashboards": [],
            "environmentMetadata": {
                "base_environment": "",
                "environment_version": "4"
            },
            "inputWidgetPreferences": null,
            "language": "python",
            "notebookMetadata": {
                "pythonIndentUnit": 4
            },
            "notebookName": "etl_m5_databricks",
            "widgets": {}
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}